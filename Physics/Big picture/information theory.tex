\documentclass[UTF8, a4paper]{ctexart}

\usepackage{geometry}
\usepackage{titling}
\usepackage{titlesec}
\usepackage{paralist}
\usepackage{footnote}
\usepackage{enumerate}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{physics}
\usepackage[colorlinks, linkcolor=black, anchorcolor=black, citecolor=black]{hyperref}

\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}
\titlespacing{\paragraph}{0pt}{1pt}{10pt}[20pt]
\setlength{\droptitle}{-5em}
\preauthor{\vspace{-10pt}\begin{center}}
\postauthor{\par\end{center}}

\newcommand*{\natnums}{\mathbb{N}}
\newcommand*{\integers}{\mathbb{Z}}
\newcommand*{\reals}{\mathbb{R}}

\newcommand*{\diff}{\mathop{}\!\mathrm{d}}
\newcommand*{\st}{\quad \text{s.t.} \quad}
\newcommand*{\const}{\mathrm{const}}
\DeclareMathOperator{\exception}{\mathbb{E}}

% \renewenvironment{itemize}{\begin{compactitem}}{\end{compactitem}}
% \renewenvironment{enumerate}{\begin{compactenum}}{\end{compactenum}}

\newenvironment{bigcase}{\left\{\quad\begin{aligned}}{\end{aligned}\right.}

\title{计算和信息}
\author{吴何友}

\begin{document}

\maketitle

符号规定：以下如无特殊说明，$\log$以$2$为底。这是不重要的，实际上完全可以使用别的底。

\section{经典信息论}

\subsection{信息熵及其衍生概念}

\subsubsection{香农信息熵}

我们知道，密度算符$\hat{\rho}$的冯诺依曼熵为
\begin{equation}
    S = - \trace (\hat{\rho} \ln \hat{\rho}),
\end{equation}
在经典情况下——也就是密度算符在某一组基底下始终是对角的，这通常是因为环境扰动导致了一组偏好基——密度算符就可以完全使用随机变量表示，而做一次观测就是从随机变量中做一个抽样。
设有离散型随机变量$X \sim p(x)$是某个系统的密度算符在经典情况下对应的随机变量，我们称它为\textbf{信源}。我们需要衡量这个随机变量携带的信息量，或者说，这个随机变量的值如果能够确定，我们可以收到多少信息。
所谓信息，就是不确定性的消除，因此我们只需要衡量这个随机变量有多混乱（也即，这个随机变量的值确定之后能够消除多少混乱），就衡量了它能够提供的信息。
这种混乱程度称为\textbf{信息熵}，它是随机变量$X$的不同取值的概率的函数，即
\[
    H = S(p_1, p_2, \ldots).
\]
具体每个$p_i$对应的$X_i$的值实际上不那么重要，因为最关键的显然是这些值能够彼此区分开来；两个$m$元集合之间可以建立一个一一对应，而直觉上它们的信息量肯定是一样的。
我们下面使用
\[
    H_m (p_1, p_2, \ldots, p_m)
\]
表示一个有$m$个不同的可能取值，第$i$个取值的概率为$p_i$的随机变量的信息熵。

当然，可以直接从玻尔兹曼熵推导出（经典的）信息熵，但是也可以通过公理化方法。
我们要求信息熵满足以下条件：
\begin{enumerate}
    \item 连续性，即$S$对各个$p_i$都是连续的。
    \item 在等概率时，即如果$X$有$N$个取值，每个取值的概率是$1/N$时，$N$越大$S$越大（这是很合理的，因为涉及越长的编码的随机变量携带的信息显然越多）。
    \item 可加性，这个性质叙述起来比较麻烦。我们把$m$个可能的结果分到$n$个不交集合中，用$I_s$表示第$s$个集合。
    显然，“观察结果落在$I_s$中”的概率为
    \[
        P(I_s) = \sum_{i \in I_s} p_i,
    \]
    且我们将集合$I_s$中的可能结果放在一起，做成一个新的随机变量，则其概率分布为
    \[
        p_{s, i} = \frac{p_i}{P(I_s)} = \frac{p_i}{\sum_{i \in I_s} p_i}.
    \]
    我们当然可以固定$s$，使用$p_{s, i}$计算信息熵，并且非常合理地，我们会要求信息熵是线性可加的，即
    \begin{equation}
        H_m(p_1, \ldots, p_m) = \sum_s P(I_s) H_{\left\| I_s \right\|} (\{p_{s, i} | i \in I_s \}).
    \end{equation}
\end{enumerate}
满足以上条件的信息熵定义，差一个常数因子，可以唯一确定为
\begin{equation}
    H(X) = - \sum_{\text{possible } x} p(x) \log p(x).
\end{equation}
很容易看出这就是玻尔兹曼熵的特例。$H(X)$不小于零，且当且仅当毫无随机性，即可以找到一个$i$使得$p_i=1$时，$H(X)=0$。

\subsubsection{联合熵、条件熵、互信息}

实际问题中涉及的信源肯定不止一个，从信息熵出发可以定义很多衡量两个信源之间的关系。
首先可以定义\textbf{联合熵}$H(X, Y)$，它是两个信源的笛卡尔积$(X, Y)$的信息熵，计算公式为
\begin{equation}
    H(X, Y) = - \sum_{x, y} p(x, y) \log p(x, y),
\end{equation}
且非常合理地，可以验证$H(X, Y) = H(Y, X)$。
假设我们首先从$X$那里获得了一些信息，然后又从$Y$那里获得了一些信息%
\footnote{在这里以及之后，如无特殊说明都假定$X, Y$等信源在两次观察之间没有发生时间演化。}%
，则从$X$那里获取的信息量为$H(X)$而先从$X$再从$Y$那里获得信息之后获得的信息量为$H(X, Y)$，则“从$X$那里获得信息后，再从$Y$那里获得的额外的信息量”——也即\textbf{条件熵}——为
\begin{equation}
    H(Y|X) = H(X, Y) - H(X),
\end{equation}
根据条件概率的定义马上可以得到
\begin{equation}
    H(Y|X) = - \sum_{x, y} p(x, y) \log p(y|x).
\end{equation}
此外，很容易看出无论条件熵还是联合熵都大于等于零，且
\begin{equation}
    0 \leq H(X|Y) \leq H(X),
\end{equation}
当且仅当$X$和$Y$分布独立时取右边等号（即知道$Y$后再测量$X$获得的纯粹就是$X$的信息），当且仅当存在双射函数$g$使得$X=g(Y)$时取左边等号（即$X$提供的信息就是$Y$提供的信息，知道$Y$后再测量$X$获得不了任何新的信息）。

条件熵和联合熵都是在分析“总的信息量”。如果我们只想知道一个信源——比如说$X$——提供的信息，那么在尚未从$Y$获得信息时，单独观测$X$能够获得信息$H(X)$，而从$Y$获得信息之后，再观测$X$获得的信息就是$H(X|Y)$，从而$Y$提供的关于$X$的信息就是
\begin{equation}
    I(X;Y) = H(X) - H(X|Y) = H(X, Y) - H(X|Y) - H(Y|X),
\end{equation}
称为\textbf{互信息}。显然
\begin{equation}
    I(X;Y) = I(Y;X),
\end{equation}
且由于
\[
    0 \leq H(X|Y) \leq H(x),
\]
我们有
\begin{equation}
    0 \leq I(X;Y) \leq \min(H(X), H(Y)),
\end{equation}
当且仅当$X$和$Y$独立时取左边等号（即$X$和$Y$对确定对方的信息毫无作用），当且仅达存在双射函数$g$使得$X=g(Y)$时取右边等号，此时
\[
    H(X) = H(Y).
\]

\subsubsection{编码和比特}

如果信源$X$可以被一个单射映射到信源$Y$上，我们就说$Y$\textbf{编码}了$X$。编码的概念是直观上非常合理的。当然，如果$X$和$Y$之间能够建立一一对应，那么它们的信息量就是完全一样的；反之，$Y$中就含有\textbf{冗余信息}，此时$Y$的信息熵大于$X$的信息熵。
总之，如果$H(Y) \geq H(X)$，那么$Y$就可以编码$X$，否则不行。
% TODO:这里缺乏一个反向的论证，即是否任意两个集合之间都可以建立某种映射；可能涉及一些集合论比较tricky的地方

我们下面寻找一种非常简单，具有非常良好的性质，然而又足够普适的编码方案。设$X$是$N$个取值的随机变量，那么就有
\[
    H(X) = - \sum_{i=1}^N p_i \log p_i, \quad \sum_i p_i = 1,
\]
容易验证让$H(X)$最大的选择是让所有事件都是等可能的，此时
\begin{equation}
    H(X) = \log N.
\end{equation}
进一步，我们假定
\[
    N = 2^M,
\]
那么$X$又可以编码为一系列彼此独立的二值随机变量的积，每一个事件都可以写成$100110\cdots$的形式，且
\[
    H(X) = M.
\]
如果不以$2$为底数上式需要乘上一个因子。现在我们看到了信息熵在编码上的含义：如果$X$可以使用$M$个彼此独立的二值随机变量（称为$n$个\textbf{比特}）的积编码，那么它的信息量就是$M$。
一般情况下，$X$的信息量不会是一个整数，设
\[
    M < H(X) < M+1,
\]
则$M$个比特不足以编码$X$，$M+1$个比特足以编码$X$但是会有冗余。

信息可以用于做功。

\section{经典计算}

\subsection{图灵机和图灵完备性}

\subsubsection{图灵机及其计算能力}

提出图灵机的最早动机是形式化人类的推理过程，从而可以建立一个能够证明一切数学定理的机器。这个设想，由于哥德尔不完备性定理，当然是失败了。但图灵机的概念一直延续了下来，并且成为理论计算机科学的基础。

一个\textbf{图灵机}是这样一个机器：它有一个无限长的纸带，一个读写头，一个控制器，还有一个指令集。
纸带被分割成一系列格子，每个格子上的内容来自一个有限的字母表；读写头在每个时刻都放在一个格子里；控制器可以取一系列状态，当它取某个特殊状态时图灵机停止工作。
纸带一开始被初始化为输入，然后图灵机重复以下步骤直到停机：
\begin{enumerate}
    \item 读写头读入当前格子上的内容$a$；
    \item 从指令集中寻找一条形如$(s, a) \longrightarrow (s', a', d)$的指令，其中$s$为当前控制器的状态（由于字母表和控制器状态都是有限的，指令集当然也是有限的）；
    \item 将当前格子中的内容变为$a'$；
    \item 将控制器设置为$s'$；
    \item 将读写头向右移动$d$个格子（如果$d<0$就向左移动）。
\end{enumerate}
停机后纸带内容就是输出。如果图灵机不停机，那么输出为空。

可以使用图灵机实现各种常见的算法。可以证明，允许多条纸带、允许多维纸带，并不改变图灵机的计算能力（如果机器$M$可以做所有机器$M'$能做的工作，反之亦然，那么$M$和$M'$就有一样的计算能力；暂时不考虑计算效率）。
乍一看，如果我们认为每个$(s, a)$可以有多个对应的$(s', a', d)$，具体转移到哪一个$(s', a', d)$中有一个概率，那么这种\textbf{概率图灵机}的行为相比确定性图灵机会非常不同，因为概率图灵机的输出不是确定性的。
但是实际上，我们总是可以使用一台确定性图灵机把概率图灵机的每一条可能走的路径都计算一遍，再计算概率图灵机每个分支的结局，只有最后输出时才取随机数输出一个结果，因此从这个意义上概率图灵机也可以被确定性图灵机模拟——概率图灵机的非确定性可以被“收集起来”，在最后一步考虑。
总之，图灵机可以计算的问题的集合似乎相当稳定：无论如何向图灵机加入新的因素，它能够解决的问题的集合都不变。

实际上，我们可以建立一台\textbf{通用图灵机}：它可以模拟任何其它的图灵机，只需要读入一台图灵机的描述（编码为一个数，称为\textbf{图灵数}）和一个输入结果，就能够完美模拟这个图灵机的行为。
设图灵机$T$对应的图灵数为$n_T$，设通用图灵机为$U$，则
\begin{equation}
    U(n_T, x) = T(x).
\end{equation}

有没有图灵机解决不了的问题？\textbf{停机问题}，即判断一台图灵机在一个给定的输入之下是不是会停机，是一个不能由图灵机一般性解决的问题。
假定存在一个图灵机$\text{halt}$，它读入一个图灵数$x$，如果停机则输出$1$，否则输出

一类常见的问题是所谓\textbf{决策问题}。% TODO
所有的决策问题都可以转化为判断一个句子是不是属于形式语言。有不可数无穷多个形式语言，但只有可数无穷多个图灵机，因此大部分决策问题都不能通过图灵机解决，虽然它们乍一看都是定义良好的。

\subsubsection{逻辑电路}

和图灵机的等价性

\subsection{算法复杂度}

\subsubsection{算法时间复杂度}

现在我们转而讨论一个稍微现实一些的问题。一个问题原则上可以解决是一回事，能不能快速地被解决则是另外一回事。
设输入大小为$n$，我们认为一个\textbf{简单}的问题需要的最小步骤数目（以下简称“需要的步骤数”）以$n$的某个多项式为上界；不简单的问题都是\textbf{苦难}的。
例如，加减乘除都是简单的：$n$位整数相加需要计算$n$次一位数相加，还有最多$n$次进位，所以需要的步骤数最多$2n$，在可接受范围内；减法同理；对乘法，$n$位整数相乘，需要计算$n$次一位数和$n$位数相乘，以及$n$次$n$位数相加，而每个一位数和$n$位数相乘的步骤需要大约$n$个步骤，于是乘法需要的步骤数量级在$n^2$。
在讨论算法复杂度时，通常用$O(f(n))$表示一个算法需要的步骤数上界是某个常数乘以$f(n)$，用$\Omega(f(n))$表示一个算法需要的步骤数下界是某个常数乘以$f(n)$，用$\Theta(f(n))$表示一个算法需要的步骤数就和$f(n)$同阶，即同时$O(f(n))$和$\Omega(f(n))$。

定义$\text{TIME}(f)$为能够在$O(f(n))$时间内用确定性图灵机解决的决策问题，$\text{NTIME}(f)$为能够在$O(f(n))$时间内用概率图灵机解决的决策问题。
定义集合$\text{P}$为所有$\text{TIME}(n^k)$的并集，$\text{NP}$为所有$\text{NTIME}(n^k)$的并集。可以证明，$\text{NP}$实际上就是确定性图灵机能够在多项式时间内验证答案是否正确的所有问题。
再设$\text{NPC}$为所有$\text{NP}$问题都可以在多项式时间内约化到其上的全体NP问题，即\textbf{NP-完全问题}（显然一个NP-完全问题可以多项式时间内约化到另一个NP-完全问题之上）。
全体$\text{NP}$问题都可以多项式时间内约化到一组问题上，这组问题可以不是NP问题，它们称为\textbf{NP-困难问题}。
NP-困难问题当然至少不比NP-完全问题简单，而NP-完全问题不比非NP-完全的NP问题简单，而NP问题也不比P问题简单。
现在的问题是，是否有$\text{P}=\text{NP}$？

多带图灵机的计算能力虽然和单带图灵机完全一样，但是它的效率比单带图灵机要高。
不过另一方面，多带图灵机的计算复杂度改善也只是多项式级别的，因此并不根本改变问题的复杂性。

\subsubsection{算法空间复杂度}

仿照定义时间复杂度的方法，还可以定义空间复杂度。
$\text{NSPACE}=\text{SPACE}$

即在空间复杂度上，确定性图灵机可以毫无困难地模拟非确定性图灵机。

\subsubsection{非确定性图灵机的误差}

BPP：经典概率图灵机正确概率大于$2/3$
BQP：量子概率图灵机正确概率大于$2/3$

\subsubsection{各复杂性类的关系}

\[
    \text{L} \subseteq \text{NL} \subseteq \text{P} \subseteq \text{NP} \subseteq \text{PSPACE} \subseteq \text{EXP}
\]

这些包含关系是不是真包含关系，即能不能取等号，仍然有很多开放问题。最为著名的就是$\text{P}=\text{NP}$问题。已经有了一些结果，如$\text{P} \neq \text{EXP}$，以及$\text{NL} \neq \text{PSPACE}$。

此外也有
\[
    \text{L} \subseteq \text{NL} \subseteq \text{P} \subseteq \text{BPP} \subseteq \text{BQP} \subseteq \text{PSPACE} \subseteq \text{EXP}
\]

\subsection{图灵机和经典物理}

\subsubsection{图灵机是否模拟了世界？}

\subsubsection{不可逆性}

常见的逻辑门——与门、或门、非门——都是不可逆的，也即从它们的输出结果没法倒推输入。
但是，物理定律一般是时间反演不变的，那么信息丢失到了哪里？我们可以认为信息丢失到了环境或是一些不可观察的内部自由度中；或者，等价地说，由于信息可以用于做功，丢失信息相当于通过某种“摩擦”（也即，和不可见自由度的耦合）把功变成了热。
正是这一步造成了熵的增大，具体可以表现为产生废热、需要额外能量输入等。
于是我们得到\textbf{Landauer原理}：抹去1比特的信息产热$k_\text{B} T \ln 2$。

这样，如果我们小心地保持通过每个逻辑门的信息都没有丢失，就可以制造出一台自身不产生任何废热的计算机，即实现\textbf{可逆计算}。

受控非门CNOT：
\[
    (a, b) \longrightarrow (a, a \oplus b)
\]

Toffoli门，或CCNOT

Toffoli门加上辅助比特就可以实现所有计算，因为它可以容易地实现NAND。实际上，可以证明，使用两比特门不能够实现普适的可逆计算。

实际上，可以验证，使用撞球计算机能够实现以上所有可逆逻辑门，因此

原则上总是可以使用不可逆的逻辑门来实现可逆的逻辑门，但是这并没有产生任何矛盾：一个原则上可以用可逆的物理过程实现的逻辑门当然也可以用不可逆的物理过程实现。

\section{量子信息}

即使物理规律是局域的，粒子的波函数$\braket*{\vb*{x}}{\psi}$也可以是非定域的。这个乍一看非常显然的结论会导致非常不同寻常的结果，即波函数可以发生超距变化。

态矢量的确会有超距变化，但是并没有信息的超距传输。这非常类似“相速度”的概念：两个相隔很远的地方依次发生两个事件，看起来似乎有什么东西在超距传播，但是实际上并没有信息的超距流动。
总之，波函数可以发生超距变化这件事意味着我们必须在实在性和定域性之间放弃一个。% TODO

\section{量子计算}

在量子计算中可逆性表现得更为明显，因为量子计算实际上就是特殊的幺正变换，而幺正变换当然是可逆的。
和我们在经典计算中的论证相似，量子计算中如果要出现不可逆性，那么一定需要有一个步骤抹去信息。
可以简单地通过和环境交互将无用的信息抹去，而容易看出实际上这就相当于进行了一次观测——可以是标准的，观测之后系统立即塌缩到一个本征态上的那种观测，也可以是某种“弱测量”。

\subsection{量子不可能性}

一些在经典情况下毫无问题的操作在量子纯态中实际上是不可能的。本节讨论这些不可能的操作。

首先我们有\textbf{量子不可复制定理}：不存在一个过程，能够将任意的态$\ket*{\phi}$复制。假定存在这样的过程$\hat{U}$，则对两个不一样的量子态$\ket{\phi_1}$和$\ket*{\phi_2}$，有
\[
    \hat{U} \ket*{\phi_1} \ket*{0} = \ket*{\phi_1} \ket*{\phi_1}, \quad \hat{U} \ket*{\phi_2} \ket*{0} = \ket*{\phi_2} \ket*{\phi_2} ,
\]
于是
\[
    \hat{U} (\ket*{\phi_1} + \ket*{\phi_2}) \ket*{0} = \ket*{\phi_1} \ket*{\phi_1} + \ket*{\phi_2} \ket*{\phi_2},
\]
但是另一方面，依照定义
\[
    \hat{U} (\ket*{\phi_1} + \ket*{\phi_2}) \ket*{0} = (\ket*{\phi_1} + \ket*{\phi_2}) (\ket*{\phi_1} + \ket*{\phi_2}),
\]
这就导致了一个矛盾。

量子不可复制定理意味着经典计算中的赋值是不可能的。

\subsection{标准测量}

本节讨论怎样在量子计算系统中做测量。

首先考虑\textbf{投影测量}。设$\hat{M}$是一个可观察量，它可以做特征分解
\begin{equation}
    \hat{M} = \sum_m m \hat{P}_m, \quad \hat{P}_m = \sum_{i \in I_m} \dyad*{i}{i},
\end{equation}
则容易验证，测量得到$m$的概率为
\begin{equation}
    P(m) = \mel*{\psi}{\hat{P}_m}{\psi},
\end{equation}
测量结束后态矢量为
\begin{equation}
    \ket{\psi'} = \frac{\hat{P}_m \ket{\psi}}{\sqrt{P(m)}}.
\end{equation}
容易验证，测量平均值和方差分别为
\begin{equation}
    \expval*{\hat{M}} = \mel*{\psi}{\hat{M}}{\psi}, \quad \abs*{\Delta_M}^2 = \expval*{\hat{M}^2} - \expval*{\hat{M}}^2 = \mel*{\psi}{\hat{M}^2}{\psi} - \mel*{\psi}{\hat{M}}{\psi}^2.
\end{equation}

Massen-

\begin{equation}
    H(A) + H(B) \geq \log d
\end{equation}

\end{document}