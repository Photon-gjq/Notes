\documentclass[UTF8, a4paper]{ctexart}

\usepackage{geometry}
\usepackage{titling}
\usepackage{titlesec}
\usepackage{paralist}
\usepackage{footnote}
\usepackage{enumerate}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{physics}
\usepackage[colorlinks, linkcolor=black, anchorcolor=black, citecolor=black]{hyperref}

\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}
\titlespacing{\paragraph}{0pt}{1pt}{10pt}[20pt]
\setlength{\droptitle}{-5em}
\preauthor{\vspace{-10pt}\begin{center}}
\postauthor{\par\end{center}}

\newcommand*{\natnums}{\mathbb{N}}
\newcommand*{\integers}{\mathbb{Z}}
\newcommand*{\reals}{\mathbb{R}}

\newcommand*{\diff}{\mathop{}\!\mathrm{d}}
\newcommand*{\st}{\quad \text{s.t.} \quad}
\newcommand*{\const}{\mathrm{const}}
\DeclareMathOperator{\exception}{\mathbb{E}}

% \renewenvironment{itemize}{\begin{compactitem}}{\end{compactitem}}
% \renewenvironment{enumerate}{\begin{compactenum}}{\end{compactenum}}

\newenvironment{bigcase}{\left\{\quad\begin{aligned}}{\end{aligned}\right.}

\title{计算和信息}
\author{吴何友}

\begin{document}

\maketitle

符号规定：以下如无特殊说明，$\log$以$2$为底。这是不重要的，实际上完全可以使用别的底。

\section{经典信息论}

\subsection{信息熵及其衍生概念}

\subsubsection{香农信息熵}

我们知道，密度算符$\hat{\rho}$的冯诺依曼熵为
\begin{equation}
    S = - \trace (\hat{\rho} \ln \hat{\rho}),
\end{equation}
在经典情况下——也就是密度算符在某一组基底下始终是对角的，这通常是因为环境扰动导致了一组偏好基——密度算符就可以完全使用随机变量表示，而做一次观测就是从随机变量中做一个抽样。
设有离散型随机变量$X \sim p(x)$是某个系统的密度算符在经典情况下对应的随机变量，我们称它为\textbf{信源}。我们需要衡量这个随机变量携带的信息量，或者说，这个随机变量的值如果能够确定，我们可以收到多少信息。
所谓信息，就是不确定性的消除，因此我们只需要衡量这个随机变量有多混乱（也即，这个随机变量的值确定之后能够消除多少混乱），就衡量了它能够提供的信息。
这种混乱程度称为\textbf{信息熵}，它是随机变量$X$的不同取值的概率的函数，即
\[
    H = S(p_1, p_2, \ldots).
\]
具体每个$p_i$对应的$X_i$的值实际上不那么重要，因为最关键的显然是这些值能够彼此区分开来；两个$m$元集合之间可以建立一个一一对应，而直觉上它们的信息量肯定是一样的。
我们下面使用
\[
    H_m (p_1, p_2, \ldots, p_m)
\]
表示一个有$m$个不同的可能取值，第$i$个取值的概率为$p_i$的随机变量的信息熵。

当然，可以直接从玻尔兹曼熵推导出（经典的）信息熵，但是也可以通过公理化方法。
我们要求信息熵满足以下条件：
\begin{enumerate}
    \item 连续性，即$S$对各个$p_i$都是连续的。
    \item 在等概率时，即如果$X$有$N$个取值，每个取值的概率是$1/N$时，$N$越大$S$越大（这是很合理的，因为涉及越长的编码的随机变量携带的信息显然越多）。
    \item 可加性，这个性质叙述起来比较麻烦。我们把$m$个可能的结果分到$n$个不交集合中，用$I_s$表示第$s$个集合。
    显然，“观察结果落在$I_s$中”的概率为
    \[
        P(I_s) = \sum_{i \in I_s} p_i,
    \]
    且我们将集合$I_s$中的可能结果放在一起，做成一个新的随机变量，则其概率分布为
    \[
        p_{s, i} = \frac{p_i}{P(I_s)} = \frac{p_i}{\sum_{i \in I_s} p_i}.
    \]
    我们当然可以固定$s$，使用$p_{s, i}$计算信息熵，并且非常合理地，我们会要求信息熵是线性可加的，即
    \begin{equation}
        H_m(p_1, \ldots, p_m) = \sum_s P(I_s) H_{\left\| I_s \right\|} (\{p_{s, i} | i \in I_s \}).
    \end{equation}
\end{enumerate}
满足以上条件的信息熵定义，差一个常数因子，可以唯一确定为
\begin{equation}
    H(X) = - \sum_{\text{possible } x} p(x) \log p(x).
\end{equation}
很容易看出这就是玻尔兹曼熵的特例。$H(X)$不小于零，且当且仅当毫无随机性，即可以找到一个$i$使得$p_i=1$时，$H(X)=0$。

\subsubsection{联合熵、条件熵、互信息}

实际问题中涉及的信源肯定不止一个，从信息熵出发可以定义很多衡量两个信源之间的关系。
首先可以定义\textbf{联合熵}$H(X, Y)$，它是两个信源的笛卡尔积$(X, Y)$的信息熵，计算公式为
\begin{equation}
    H(X, Y) = - \sum_{x, y} p(x, y) \log p(x, y),
\end{equation}
且非常合理地，可以验证$H(X, Y) = H(Y, X)$。
假设我们首先从$X$那里获得了一些信息，然后又从$Y$那里获得了一些信息%
\footnote{在这里以及之后，如无特殊说明都假定$X, Y$等信源在两次观察之间没有发生时间演化。}%
，则从$X$那里获取的信息量为$H(X)$而先从$X$再从$Y$那里获得信息之后获得的信息量为$H(X, Y)$，则“从$X$那里获得信息后，再从$Y$那里获得的额外的信息量”——也即\textbf{条件熵}——为
\begin{equation}
    H(Y|X) = H(X, Y) - H(X),
\end{equation}
根据条件概率的定义马上可以得到
\begin{equation}
    H(Y|X) = - \sum_{x, y} p(x, y) \log p(y|x).
\end{equation}
此外，很容易看出无论条件熵还是联合熵都大于等于零，且
\begin{equation}
    0 \leq H(X|Y) \leq H(X),
\end{equation}
当且仅当$X$和$Y$分布独立时取右边等号（即知道$Y$后再测量$X$获得的纯粹就是$X$的信息），当且仅当存在双射函数$g$使得$X=g(Y)$时取左边等号（即$X$提供的信息就是$Y$提供的信息，知道$Y$后再测量$X$获得不了任何新的信息）。

条件熵和联合熵都是在分析“总的信息量”。如果我们只想知道一个信源——比如说$X$——提供的信息，那么在尚未从$Y$获得信息时，单独观测$X$能够获得信息$H(X)$，而从$Y$获得信息之后，再观测$X$获得的信息就是$H(X|Y)$，从而$Y$提供的关于$X$的信息就是
\begin{equation}
    I(X;Y) = H(X) - H(X|Y) = H(X, Y) - H(X|Y) - H(Y|X),
\end{equation}
称为\textbf{互信息}。显然
\begin{equation}
    I(X;Y) = I(Y;X),
\end{equation}
且由于
\[
    0 \leq H(X|Y) \leq H(x),
\]
我们有
\begin{equation}
    0 \leq I(X;Y) \leq \min(H(X), H(Y)),
\end{equation}
当且仅当$X$和$Y$独立时取左边等号（即$X$和$Y$对确定对方的信息毫无作用），当且仅达存在双射函数$g$使得$X=g(Y)$时取右边等号，此时
\[
    H(X) = H(Y).
\]

\subsubsection{编码和比特}

如果信源$X$可以被一个单射映射到信源$Y$上，我们就说$Y$\textbf{编码}了$X$。编码的概念是直观上非常合理的。当然，如果$X$和$Y$之间能够建立一一对应，那么它们的信息量就是完全一样的；反之，$Y$中就含有\textbf{冗余信息}，此时$Y$的信息熵大于$X$的信息熵。
总之，如果$H(Y) \geq H(X)$，那么$Y$就可以编码$X$，否则不行。
% TODO:这里缺乏一个反向的论证，即是否任意两个集合之间都可以建立某种映射；可能涉及一些集合论比较tricky的地方

我们下面寻找一种非常简单，具有非常良好的性质，然而又足够普适的编码方案。设$X$是$N$个取值的随机变量，那么就有
\[
    H(X) = - \sum_{i=1}^N p_i \log p_i, \quad \sum_i p_i = 1,
\]
容易验证让$H(X)$最大的选择是让所有事件都是等可能的，此时
\begin{equation}
    H(X) = \log N.
\end{equation}
进一步，我们假定
\[
    N = 2^M,
\]
那么$X$又可以编码为一系列彼此独立的二值随机变量的积，每一个事件都可以写成$100110\cdots$的形式，且
\[
    H(X) = M.
\]
如果不以$2$为底数上式需要乘上一个因子。现在我们看到了信息熵在编码上的含义：如果$X$可以使用$M$个彼此独立的二值随机变量（称为$n$个\textbf{比特}）的积编码，那么它的信息量就是$M$。
一般情况下，$X$的信息量不会是一个整数，设
\[
    M < H(X) < M+1,
\]
则$M$个比特不足以编码$X$，$M+1$个比特足以编码$X$但是会有冗余。

\section{经典计算}

\subsection{不可逆性}

常见的逻辑门——与门、或门、非门——都是不可逆的，也即从它们的输出结果没法倒推输入。
但是，物理定律一般是时间反演不变的，那么信息丢失到了哪里？我们可以认为信息丢失到了环境中。换句话说，逻辑门的输出需要抹去一些信息，而由Landauer原理正是这一步造成了熵的增大，具体可以表现为产生废热、需要额外能量输入等。

\section{量子信息}

即使物理规律是局域的，粒子的波函数$\braket*{\vb*{x}}{\psi}$也可以是非定域的。这个乍一看非常显然的结论会导致非常不同寻常的结果，即波函数可以发生超距变化。

态矢量的确会有超距变化，但是并没有信息的超距传输。这非常类似“相速度”的概念：两个相隔很远的地方依次发生两个事件，看起来似乎有什么东西在超距传播，但是实际上并没有信息的超距流动。
总之，波函数可以发生超距变化这件事意味着我们必须在实在性和定域性之间放弃一个。% TODO

\section{量子计算}

在量子计算中可逆性表现得更为明显，因为量子计算实际上就是特殊的幺正变换，而幺正变换当然是可逆的。
和我们在经典计算中的论证相似，量子计算中如果要出现不可逆性，那么一定需要有一个步骤抹去信息。
可以简单地通过和环境交互将无用的信息抹去，而容易看出实际上这就相当于进行了一次观测——可以是标准的，观测之后系统立即塌缩到一个本征态上的那种观测，也可以是某种“弱测量”。

\end{document}